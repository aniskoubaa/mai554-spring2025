{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "850bc77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nMachine Translation Tutorial with Pretrained Transformers\\nMAI554 Deep Learning Course\\n\\nThis script demonstrates how to use Hugging Face's pretrained transformer models for\\nmachine translation and how to evaluate the results using BLEU score.\\n\\nThis can be run as a Python script or converted to a Jupyter notebook.\\nTo convert to a notebook, run:\\n    jupytext --to notebook machine_translation_tutorial.py\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Machine Translation Tutorial with Pretrained Transformers\n",
    "MAI554 Deep Learning Course\n",
    "\n",
    "This script demonstrates how to use Hugging Face's pretrained transformer models for\n",
    "machine translation and how to evaluate the results using BLEU score.\n",
    "\n",
    "This can be run as a Python script or converted to a Jupyter notebook.\n",
    "To convert to a notebook, run:\n",
    "    jupytext --to notebook machine_translation_tutorial.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae1654",
   "metadata": {},
   "source": [
    "# 🌍 Machine Translation with Pretrained Transformers 🤖\n",
    "\n",
    "This notebook demonstrates how to use Hugging Face's pretrained transformer models for machine translation \n",
    "and how to evaluate the results using BLEU score.\n",
    "\n",
    "## 📚 Overview\n",
    "\n",
    "In this tutorial, we will:\n",
    "1. Load a pretrained transformer model for translation\n",
    "2. Translate text from one language to another\n",
    "3. Evaluate the translation quality using BLEU score\n",
    "4. Visualize the results\n",
    "5. Learn how to push models to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603bd84",
   "metadata": {},
   "source": [
    "## 📦 Install Required Packages\n",
    "\n",
    "Let's start by installing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439ad283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (0.30.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacremoses in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from huggingface-hub) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/mai544/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run this cell if you need to install the packages\n",
    "!pip install transformers datasets nltk matplotlib tqdm huggingface-hub sentencepiece sacremoses torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c4c72",
   "metadata": {},
   "source": [
    "## 🚀 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9512f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK punkt tokenizer data already exists.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Make sure NLTK data is properly downloaded\n",
    "# First, check if the directory exists\n",
    "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "# Check if punkt is already downloaded\n",
    "punkt_dir = os.path.join(nltk_data_dir, 'tokenizers', 'punkt')\n",
    "if not os.path.exists(punkt_dir):\n",
    "    print(\"Downloading NLTK punkt tokenizer data...\")\n",
    "    nltk.download('punkt', download_dir=nltk_data_dir, quiet=False)\n",
    "else:\n",
    "    print(\"NLTK punkt tokenizer data already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f758a1",
   "metadata": {},
   "source": [
    "## 🔄 Loading a Translation Model\n",
    "\n",
    "We'll use Hugging Face's `pipeline` API to load a pretrained English-to-German translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77bfcbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model Helsinki-NLP/opus-mt-en-de for translation from en to de\n"
     ]
    }
   ],
   "source": [
    "# Define language parameters\n",
    "source_lang = 'en'\n",
    "target_lang = 'de'\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
    "\n",
    "# Load the translation model\n",
    "translator = pipeline(f\"translation_{source_lang}_to_{target_lang}\", model=model_name)\n",
    "print(f\"✅ Loaded model {model_name} for translation from {source_lang} to {target_lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5faac",
   "metadata": {},
   "source": [
    "## 🌐 Basic Translation Example\n",
    "\n",
    "Let's try translating a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26e75fa4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Machine translation is transforming natural language processing.\n",
      "German: Maschinelle Übersetzung transformiert die natürliche Sprachverarbeitung.\n"
     ]
    }
   ],
   "source": [
    "# Define a sample sentence\n",
    "source_sentence = \"Machine translation is transforming natural language processing.\"\n",
    "\n",
    "# Translate the sentence\n",
    "result = translator(source_sentence, max_length=60)\n",
    "translated_text = result[0]['translation_text']\n",
    "\n",
    "# Display results\n",
    "print(f\"English: {source_sentence}\")\n",
    "print(f\"German: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812b6ed",
   "metadata": {},
   "source": [
    "## 📊 Evaluating Translation Quality with BLEU\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is a metric for evaluating machine translations. It compares \n",
    "a machine translation output against one or more reference translations and computes a score based \n",
    "on n-gram precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d09e4d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ NLTK data not found. Attempting to download again...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/akoubaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/akoubaa/nltk_data'\n    - '/opt/anaconda3/envs/mai544/nltk_data'\n    - '/opt/anaconda3/envs/mai544/share/nltk_data'\n    - '/opt/anaconda3/envs/mai544/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcompute_bleu_score\u001b[39m\u001b[34m(reference, hypothesis)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Tokenize the sentences into words\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     reference_tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m:type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03mA constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03ma lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m:type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1743\u001b[39m PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/akoubaa/nltk_data'\n    - '/opt/anaconda3/envs/mai544/nltk_data'\n    - '/opt/anaconda3/envs/mai544/share/nltk_data'\n    - '/opt/anaconda3/envs/mai544/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m translation = result[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtranslation_text\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Compute BLEU score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m bleu = \u001b[43mcompute_bleu_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSource:      \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mcompute_bleu_score\u001b[39m\u001b[34m(reference, hypothesis)\u001b[39m\n\u001b[32m     13\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m, download_dir=nltk_data_dir, quiet=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Try again after downloading\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m reference_tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sentence_bleu([reference_tokens], hypothesis_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mai544/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/akoubaa/nltk_data'\n    - '/opt/anaconda3/envs/mai544/nltk_data'\n    - '/opt/anaconda3/envs/mai544/share/nltk_data'\n    - '/opt/anaconda3/envs/mai544/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def compute_bleu_score(reference, hypothesis):\n",
    "    \"\"\"Compute BLEU score between reference and hypothesis\"\"\"\n",
    "    try:\n",
    "        # Tokenize the sentences into words\n",
    "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        return sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "    except LookupError:\n",
    "        # If NLTK data is not found, try downloading it again\n",
    "        print(\"⚠️ NLTK data not found. Attempting to download again...\")\n",
    "        nltk.download('punkt', download_dir=nltk_data_dir, quiet=False)\n",
    "        # Try again after downloading\n",
    "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "        return sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "\n",
    "# Define a source sentence and its reference translation\n",
    "source = \"Machine translation is transforming natural language processing.\"\n",
    "reference = \"Die maschinelle Übersetzung verändert die Verarbeitung natürlicher Sprache.\"\n",
    "\n",
    "# Translate the source text\n",
    "result = translator(source, max_length=100)\n",
    "translation = result[0]['translation_text']\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu = compute_bleu_score(reference, translation)\n",
    "\n",
    "# Display results\n",
    "print(f\"Source:      {source}\")\n",
    "print(f\"Reference:   {reference}\")\n",
    "print(f\"Translation: {translation}\")\n",
    "print(f\"BLEU Score:  {bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b029188",
   "metadata": {},
   "source": [
    "## 🧪 Multiple Examples and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source sentences and their reference translations\n",
    "examples = [\n",
    "    (\n",
    "        \"Machine translation is transforming natural language processing.\",\n",
    "        \"Die maschinelle Übersetzung verändert die Verarbeitung natürlicher Sprache.\"\n",
    "    ),\n",
    "    (\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Der schnelle braune Fuchs springt über den faulen Hund.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Artificial intelligence is the future of technology.\",\n",
    "        \"Künstliche Intelligenz ist die Zukunft der Technologie.\"\n",
    "    ),\n",
    "    (\n",
    "        \"I love learning about deep learning and neural networks.\",\n",
    "        \"Ich liebe es, über Deep Learning und neuronale Netze zu lernen.\"\n",
    "    ),\n",
    "    (\n",
    "        \"The weather is beautiful today.\",\n",
    "        \"Das Wetter ist heute schön.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "results = []\n",
    "bleu_scores = []\n",
    "\n",
    "print(\"🔄 Translating examples...\")\n",
    "for source_text, reference_text in tqdm(examples):\n",
    "    # Translate the source text\n",
    "    translation_output = translator(source_text, max_length=100)\n",
    "    translated_text = translation_output[0]['translation_text']\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu = compute_bleu_score(reference_text, translated_text)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'source': source_text,\n",
    "        'reference': reference_text,\n",
    "        'translation': translated_text,\n",
    "        'bleu': bleu\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(\"\\n🔍 Translation Results:\")\n",
    "print(\"=\" * 80)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Source:     {result['source']}\")\n",
    "    print(f\"Reference:  {result['reference']}\")\n",
    "    print(f\"Translation: {result['translation']}\")\n",
    "    print(f\"BLEU Score: {result['bleu']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Calculate and display average BLEU score\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "print(f\"📈 Average BLEU Score: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffeb5b",
   "metadata": {},
   "source": [
    "## 📊 Visualizing BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BLEU scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(bleu_scores)), bleu_scores, alpha=0.7, color='skyblue')\n",
    "plt.axhline(y=avg_bleu, color='red', linestyle='--', label=f'Mean BLEU: {avg_bleu:.4f}')\n",
    "plt.xlabel('Example')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.title('BLEU Scores by Example')\n",
    "plt.xticks(range(len(bleu_scores)), [f'Example {i+1}' for i in range(len(bleu_scores))])\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('bleu_scores_by_example.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab790f",
   "metadata": {},
   "source": [
    "## 🔍 Using a Different Language Pair\n",
    "\n",
    "Let's try a different language pair. Here we'll translate from English to French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new language parameters\n",
    "new_source_lang = 'en'\n",
    "new_target_lang = 'fr'\n",
    "new_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "\n",
    "# Load the new translation model\n",
    "translator_fr = pipeline(f\"translation_{new_source_lang}_to_{new_target_lang}\", model=new_model_name)\n",
    "print(f\"✅ Loaded model {new_model_name} for translation from {new_source_lang} to {new_target_lang}\")\n",
    "\n",
    "# Define an example sentence\n",
    "source_sentence = \"Artificial intelligence is revolutionizing the field of natural language processing.\"\n",
    "\n",
    "# Translate the sentence\n",
    "result = translator_fr(source_sentence, max_length=100)\n",
    "translated_text = result[0]['translation_text']\n",
    "\n",
    "# Display results\n",
    "print(f\"English: {source_sentence}\")\n",
    "print(f\"French: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3196b8",
   "metadata": {},
   "source": [
    "## 🚀 Pushing Models to Hugging Face Hub\n",
    "\n",
    "If you want to share your model with others or use it in production, you can push it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run these cells to push a model to the Hugging Face Hub\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
    "# hub_model_id = 'YOUR_USERNAME/YOUR_MODEL_NAME'  # Change this to your desired model ID\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Push to Hub\n",
    "# model.push_to_hub(hub_model_id)\n",
    "# tokenizer.push_to_hub(hub_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c59e7a",
   "metadata": {},
   "source": [
    "## 🎓 Conclusion\n",
    "\n",
    "In this tutorial, we've learned how to:\n",
    "- Use pretrained transformer models for machine translation\n",
    "- Evaluate translation quality using BLEU score\n",
    "- Work with different language pairs\n",
    "- Push models to the Hugging Face Hub\n",
    "\n",
    "This demonstrates the power of modern Neural Machine Translation (NMT) systems that leverage transformer architectures. Try experimenting with different language pairs and models to see how the quality varies!\n",
    "\n",
    "## 📌 Next Steps\n",
    "\n",
    "1. Try using different models for the same language pair\n",
    "2. Compare different evaluation metrics\n",
    "3. Fine-tune a model on domain-specific data\n",
    "4. Create a simple web interface for your translator "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
