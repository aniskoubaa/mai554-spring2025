{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65979a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Translation Model Architecture Exploration\n",
    "MAI554 Deep Learning Course\n",
    "\n",
    "This notebook examines the architecture of pretrained translation models,\n",
    "specifically focusing on transformer-based models available in Hugging Face.\n",
    "\n",
    "This can be run as a Python script or converted to a Jupyter notebook:\n",
    "    jupytext --to notebook translation_model_architecture.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541e90f",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Understanding Translation Model Architectures üîç\n",
    "\n",
    "This notebook explores the architecture of modern neural machine translation (NMT) models, focusing on \n",
    "transformer-based models from Hugging Face. We'll load popular translation models, examine their \n",
    "architecture details, and gain insights into how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc853522",
   "metadata": {},
   "source": [
    "## üì¶ Installation and Setup\n",
    "\n",
    "First, let's install the necessary packages if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1578f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you need to install the packages\n",
    "!pip install transformers torch datasets nltk matplotlib torchinfo torchviz graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177ed03",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35da332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoTokenizer,\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from torchinfo import summary\n",
    "import os\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make sure NLTK data is properly downloaded\n",
    "# First, check if the directory exists\n",
    "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "# Check if punkt is already downloaded\n",
    "punkt_dir = os.path.join(nltk_data_dir, 'tokenizers', 'punkt')\n",
    "if not os.path.exists(punkt_dir):\n",
    "    print(\"Downloading NLTK punkt tokenizer data...\")\n",
    "    nltk.download('punkt', download_dir=nltk_data_dir, quiet=False)\n",
    "else:\n",
    "    print(\"NLTK punkt tokenizer data already exists.\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe0fb8",
   "metadata": {},
   "source": [
    "## üß© Different Types of Translation Models\n",
    "\n",
    "Before we dive into specific architectures, let's understand the main types of translation models in the Hugging Face ecosystem:\n",
    "\n",
    "1. **MarianMT**: Based on the Marian Neural Machine Translation framework, these are production-ready, lightweight models trained for specific language pairs. Example: \"Helsinki-NLP/opus-mt-en-de\"\n",
    "\n",
    "2. **BART**: BART (Bidirectional and Auto-Regressive Transformers) is a denoising autoencoder for pretraining sequence-to-sequence models. Example: \"facebook/bart-large\"\n",
    "\n",
    "3. **T5**: Text-to-Text Transfer Transformer is a unified framework that converts all NLP tasks to a text-to-text format. Example: \"t5-base\"\n",
    "\n",
    "4. **mBART**: Multilingual BART trained on many languages at once. Example: \"facebook/mbart-large-50\"\n",
    "\n",
    "5. **M2M100**: Many-to-many multilingual translation model supporting 100 languages. Example: \"facebook/m2m100_418M\"\n",
    "\n",
    "Let's explore a couple of these architectures in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918636e6",
   "metadata": {},
   "source": [
    "## üîç Model 1: MarianMT (Helsinki-NLP/opus-mt-en-de)\n",
    "\n",
    "We'll start with the MarianMT model, which is commonly used for specific language pairs. Let's load the English-to-German model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14972f0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the MarianMT model for English to German translation\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "marian_model = MarianMTModel.from_pretrained(model_name)\n",
    "marian_tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c030d",
   "metadata": {},
   "source": [
    "### üìä Model Architecture Overview\n",
    "\n",
    "Let's examine the general architecture of the MarianMT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed27ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print model architecture at a high level\n",
    "def print_model_architecture(model):\n",
    "    print(\"\\n\".join([f\"- {name}\" for name, _ in model.named_children()]))\n",
    "\n",
    "print(\"üîç High-level components of the MarianMT model:\")\n",
    "print_model_architecture(marian_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27cc3bf",
   "metadata": {},
   "source": [
    "### üß† Understanding the Transformer Architecture\n",
    "\n",
    "The MarianMT model is based on the Transformer architecture, which consists of:\n",
    "\n",
    "1. **Encoder**: Processes the input sequence (source language)\n",
    "   - **Self-Attention Layers**: Allow the model to weigh the importance of different words in the input\n",
    "   - **Feed-Forward Networks**: Further process the self-attention output\n",
    "\n",
    "2. **Decoder**: Generates the output sequence (target language)\n",
    "   - **Self-Attention Layers**: Like in the encoder, but masked to prevent looking at future tokens\n",
    "   - **Encoder-Decoder Attention**: Allows the decoder to focus on relevant parts of the encoder's output\n",
    "   - **Feed-Forward Networks**: Process the attention outputs\n",
    "\n",
    "3. **Embedding Layers**: Convert tokens to vectors for both encoder and decoder\n",
    "\n",
    "4. **Output Layer**: Converts decoder outputs to probability distributions over the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6b67d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Let's examine the encoder and decoder architecture more closely\n",
    "if hasattr(marian_model, 'encoder'):\n",
    "    print(\"\\nüîç Encoder Architecture:\")\n",
    "    print_model_architecture(marian_model.encoder)\n",
    "    \n",
    "    print(\"\\nüîç Decoder Architecture:\")\n",
    "    print_model_architecture(marian_model.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a2d63",
   "metadata": {},
   "source": [
    "### üìà Model Details and Parameters\n",
    "\n",
    "Let's get a summary of the model's size and complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(marian_model)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd104b4",
   "metadata": {},
   "source": [
    "### üîÑ Example Translation Process\n",
    "\n",
    "Let's see how a sentence flows through the model from input to translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63ab25",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define a sample input sentence\n",
    "input_text = \"Machine translation is transforming natural language processing.\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = marian_tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Tokenized input (ids): {input_ids[0].tolist()}\")\n",
    "print(f\"Tokens: {marian_tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
    "\n",
    "# Generate translation\n",
    "with torch.no_grad():\n",
    "    output_ids = marian_model.generate(input_ids, max_length=50)\n",
    "\n",
    "# Decode the output tokens\n",
    "translated_text = marian_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(f\"\\nTranslated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc4668",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Detailed Look at Attention Mechanism\n",
    "\n",
    "The heart of the Transformer architecture is the attention mechanism. Let's understand how it works:\n",
    "\n",
    "1. **Query, Key, Value Concept**: Each token produces three vectors (Q, K, V)\n",
    "2. **Attention Weights**: Computed by comparing a token's Query with all Keys\n",
    "3. **Weighted Sum**: The final output is a weighted sum of Values based on attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simplified visualization of the self-attention process\n",
    "\n",
    "def visualize_attention(tokens, attention_matrix=None):\n",
    "    \"\"\"Create a simple visualization of attention weights between tokens\"\"\"\n",
    "    if attention_matrix is None:\n",
    "        # Create a dummy attention matrix if none is provided\n",
    "        n = len(tokens)\n",
    "        attention_matrix = np.random.rand(n, n)\n",
    "        # Normalize rows to simulate softmax\n",
    "        attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(attention_matrix, cmap='viridis')\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xticks(np.arange(len(tokens)))\n",
    "    ax.set_yticks(np.arange(len(tokens)))\n",
    "    ax.set_xticklabels(tokens)\n",
    "    ax.set_yticklabels(tokens)\n",
    "    \n",
    "    # Rotate the labels and set alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"Attention Weight\", rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title(\"Self-Attention Visualization\")\n",
    "    ax.set_xlabel(\"Key Tokens\")\n",
    "    ax.set_ylabel(\"Query Tokens\")\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            text = ax.text(j, i, f\"{attention_matrix[i, j]:.2f}\",\n",
    "                          ha=\"center\", va=\"center\", color=\"w\" if attention_matrix[i, j] > 0.5 else \"black\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize with a sample attention matrix for the tokenized input\n",
    "tokens = marian_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "visualize_attention(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ea0d3",
   "metadata": {},
   "source": [
    "## üîç Model 2: T5 Model\n",
    "\n",
    "Next, let's examine the T5 (Text-to-Text Transfer Transformer) model, which takes a different approach to translation by treating all NLP tasks as text-to-text problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf59f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T5 model\n",
    "t5_model_name = \"t5-base\"\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "\n",
    "print(f\"Model loaded: {t5_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5129569",
   "metadata": {},
   "source": [
    "### üìä T5 Architecture Overview\n",
    "\n",
    "Now let's look at the high-level architecture of the T5 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç High-level components of the T5 model:\")\n",
    "print_model_architecture(t5_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff3ab0",
   "metadata": {},
   "source": [
    "### üìè Comparing T5 with MarianMT\n",
    "\n",
    "The key differences between T5 and MarianMT:\n",
    "\n",
    "1. **Task Formulation**:\n",
    "   - T5 treats all NLP tasks as text-to-text problems\n",
    "   - MarianMT is specifically designed for machine translation\n",
    "\n",
    "2. **Architecture**:\n",
    "   - Both use transformer encoders and decoders, but with different implementations\n",
    "   - T5 uses relative positional embeddings instead of absolute positions\n",
    "\n",
    "3. **Usage for Translation**:\n",
    "   - With T5, you need to prefix the input with \"translate English to German: \"\n",
    "   - MarianMT is used directly without special prefixes\n",
    "\n",
    "4. **Pre-training Objectives**:\n",
    "   - T5 was pre-trained with a masked language modeling objective (similar to BERT)\n",
    "   - MarianMT was trained specifically on parallel translation corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's count the parameters in T5\n",
    "t5_params = count_parameters(t5_model)\n",
    "print(f\"T5 model trainable parameters: {t5_params:,}\")\n",
    "print(f\"MarianMT model trainable parameters: {total_params:,}\")\n",
    "print(f\"Ratio of T5 to MarianMT parameters: {t5_params/total_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e55ff5",
   "metadata": {},
   "source": [
    "### üéØ Using T5 for Translation\n",
    "\n",
    "Let's see how to use T5 for translation, which requires prefixing the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1a6e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Prepare input for T5 (needs the task prefix)\n",
    "t5_input_text = \"translate English to German: \" + input_text\n",
    "\n",
    "# Tokenize input for T5\n",
    "t5_inputs = t5_tokenizer(t5_input_text, return_tensors=\"pt\")\n",
    "t5_input_ids = t5_inputs[\"input_ids\"]\n",
    "\n",
    "print(f\"T5 input text: {t5_input_text}\")\n",
    "print(f\"T5 tokenized input (ids): {t5_input_ids[0].tolist()}\")\n",
    "print(f\"T5 tokens: {t5_tokenizer.convert_ids_to_tokens(t5_input_ids[0])}\")\n",
    "\n",
    "# Generate translation with T5\n",
    "with torch.no_grad():\n",
    "    t5_output_ids = t5_model.generate(t5_input_ids, max_length=50)\n",
    "\n",
    "# Decode the output tokens\n",
    "t5_translated_text = t5_tokenizer.decode(t5_output_ids[0], skip_special_tokens=True)\n",
    "print(f\"\\nT5 translated text: {t5_translated_text}\")\n",
    "\n",
    "# Compare with MarianMT translation\n",
    "print(f\"MarianMT translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7aea4",
   "metadata": {},
   "source": [
    "## üß© The Building Blocks: Transformer Layers\n",
    "\n",
    "Let's look deeper at the components that make up these translation models. All transformer-based models follow a similar structure but with variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006593f3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to look at the architecture of a specific layer\n",
    "def explore_layer(model, layer_name):\n",
    "    layer = None\n",
    "    for name, module in model.named_modules():\n",
    "        if layer_name in name:\n",
    "            layer = module\n",
    "            break\n",
    "    \n",
    "    if layer:\n",
    "        print(f\"Architecture of {layer_name}:\")\n",
    "        for name, _ in layer.named_children():\n",
    "            print(f\"  - {name}\")\n",
    "    else:\n",
    "        print(f\"Layer {layer_name} not found\")\n",
    "\n",
    "# Let's examine an encoder layer from MarianMT\n",
    "explore_layer(marian_model, \"encoder.layers.0\")\n",
    "\n",
    "# And a decoder layer\n",
    "explore_layer(marian_model, \"decoder.layers.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91a6a4",
   "metadata": {},
   "source": [
    "### üîÑ Forward Pass in Translation Models\n",
    "\n",
    "Here's a high-level diagram of how data flows through a translation model:\n",
    "\n",
    "```\n",
    "Source Text ‚Üí Tokenization ‚Üí Embeddings ‚Üí Encoder ‚Üí \n",
    "Decoder (with Encoder Attention) ‚Üí Output Layer ‚Üí Detokenization ‚Üí Target Text\n",
    "```\n",
    "\n",
    "Let's visualize this flow with our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48924299",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simple visualization of the translation flow\n",
    "def visualize_translation_flow():\n",
    "    steps = [\n",
    "        \"Source Text\", \n",
    "        \"Tokenization\", \n",
    "        \"Embeddings\", \n",
    "        \"Encoder\\n(Self-Attention + FFN)\", \n",
    "        \"Decoder\\n(Self-Attention + \\nEncoder-Attention + FFN)\", \n",
    "        \"Output Projection\", \n",
    "        \"Target Text\"\n",
    "    ]\n",
    "    \n",
    "    example_values = [\n",
    "        \"Machine translation is transforming NLP.\",\n",
    "        \"[Machine, trans@@, lation, is, transform@@, ing, NLP, .]\",\n",
    "        \"[Embedding Vectors: 512D]\",\n",
    "        \"[Contextualized Representations]\",\n",
    "        \"[Generated Token Representations]\",\n",
    "        \"[Vocabulary Probabilities]\",\n",
    "        \"Die maschinelle √úbersetzung ver√§ndert die NLP.\"\n",
    "    ]\n",
    "    \n",
    "    # Create flow diagram\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    y_positions = [0] * len(steps)\n",
    "    x_positions = list(range(len(steps)))\n",
    "    \n",
    "    # Draw boxes and arrows\n",
    "    for i, (step, value) in enumerate(zip(steps, example_values)):\n",
    "        # Draw box\n",
    "        rect = plt.Rectangle((i-0.4, -0.4), 0.8, 0.8, fill=True, color='skyblue', alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(i, 0, step, ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        # Draw example value\n",
    "        ax.text(i, -0.6, value, ha='center', va='top', fontsize=9, wrap=True)\n",
    "        \n",
    "        # Draw arrow\n",
    "        if i < len(steps) - 1:\n",
    "            ax.arrow(i+0.45, 0, 0.1, 0, head_width=0.1, head_length=0.05, fc='black', ec='black')\n",
    "    \n",
    "    ax.set_xlim(-0.5, len(steps) - 0.5)\n",
    "    ax.set_ylim(-1.5, 0.5)\n",
    "    ax.set_title('Translation Model Forward Pass Flow')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_translation_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b749de",
   "metadata": {},
   "source": [
    "## üßÆ Encoding and Decoding Process\n",
    "\n",
    "Let's break down the encoding and decoding processes in more detail:\n",
    "\n",
    "### Encoding Process:\n",
    "1. **Tokenization**: Split text into subword tokens\n",
    "2. **Embedding**: Convert tokens to vectors\n",
    "3. **Positional Encoding**: Add position information\n",
    "4. **Self-Attention**: Capture relationships between tokens\n",
    "5. **Feed-Forward Network**: Process each position independently\n",
    "\n",
    "### Decoding Process:\n",
    "1. **Auto-regressive Generation**: Generate one token at a time\n",
    "2. **Masked Self-Attention**: Prevent looking at future tokens\n",
    "3. **Encoder-Decoder Attention**: Focus on relevant parts of the source\n",
    "4. **Output Projection**: Convert to vocabulary probabilities\n",
    "5. **Beam Search**: Explore multiple translation possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65f432",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Let's see how beam search affects translation quality with MarianMT\n",
    "def compare_beam_search_sizes(model, tokenizer, input_text, beam_sizes=[1, 3, 5]):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    translations = {}\n",
    "    for beam_size in beam_sizes:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                max_length=50,\n",
    "                num_beams=beam_size,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        translations[beam_size] = translation\n",
    "    \n",
    "    return translations\n",
    "\n",
    "beam_search_results = compare_beam_search_sizes(marian_model, marian_tokenizer, input_text)\n",
    "\n",
    "print(\"üîç Effect of beam search size on translation:\")\n",
    "for beam_size, translation in beam_search_results.items():\n",
    "    print(f\"\\nBeam size {beam_size}: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e226e1",
   "metadata": {},
   "source": [
    "## üåü Key Innovations in Translation Models\n",
    "\n",
    "Modern translation models have several innovations that improve quality:\n",
    "\n",
    "1. **Subword Tokenization**: Handles rare words and morphologically rich languages\n",
    "2. **Multi-head Attention**: Captures different types of relationships\n",
    "3. **Layer Normalization**: Stabilizes training\n",
    "4. **Residual Connections**: Helps with gradient flow\n",
    "5. **Beam Search Decoding**: Explores multiple translation candidates\n",
    "6. **Shared Embeddings**: Reuses knowledge between languages\n",
    "7. **Byte-Pair Encoding (BPE)**: Effective subword segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the tokenization process in more detail\n",
    "def explore_tokenization(tokenizer, text):\n",
    "    # Get subword tokens\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Get IDs\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    \n",
    "    # Display the tokenization\n",
    "    print(f\"Original text: {text}\")\n",
    "    print(f\"Tokenized into {len(tokens)} tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {input_ids}\")\n",
    "    \n",
    "    # Visualize the subword breakdown\n",
    "    words = text.split()\n",
    "    subwords_by_word = []\n",
    "    \n",
    "    token_idx = 0\n",
    "    for word in words:\n",
    "        word_tokens = []\n",
    "        word_len = len(word)\n",
    "        \n",
    "        # Find tokens that belong to this word\n",
    "        # Note: This is approximate as tokenizers may behave differently\n",
    "        while token_idx < len(tokens) and (word.startswith(tokens[token_idx].replace('‚ñÅ', '')) or \n",
    "                                          tokens[token_idx].replace('‚ñÅ', '').startswith(word) or\n",
    "                                          '@@' in tokens[token_idx]):\n",
    "            word_tokens.append(tokens[token_idx])\n",
    "            token_idx += 1\n",
    "            \n",
    "            # Break if we've collected enough tokens for this word\n",
    "            if ''.join(t.replace('‚ñÅ', '').replace('@@', '') for t in word_tokens) == word or \\\n",
    "               ''.join(t.replace('‚ñÅ', '').replace('@@', '') for t in word_tokens).startswith(word):\n",
    "                break\n",
    "        \n",
    "        subwords_by_word.append((word, word_tokens))\n",
    "    \n",
    "    # Plot the breakdown\n",
    "    fig, ax = plt.subplots(figsize=(14, len(words) * 0.8))\n",
    "    \n",
    "    for i, (word, subwords) in enumerate(subwords_by_word):\n",
    "        ax.text(0, -i, word, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        x_offset = 0.5\n",
    "        for j, subword in enumerate(subwords):\n",
    "            subword_display = subword.replace('‚ñÅ', '').replace('@@', '')\n",
    "            width = len(subword_display) * 0.15\n",
    "            \n",
    "            rect = plt.Rectangle((x_offset, -i - 0.4), width, 0.8, \n",
    "                                 fill=True, color=f'C{j%10}', alpha=0.7)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x_offset + width/2, -i, subword, \n",
    "                    ha='center', va='center', fontsize=10)\n",
    "            \n",
    "            x_offset += width + 0.1\n",
    "    \n",
    "    ax.set_xlim(0, x_offset)\n",
    "    ax.set_ylim(-len(words) - 0.5, 0.5)\n",
    "    ax.set_title('Subword Tokenization Breakdown')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Check how the sample sentence gets tokenized\n",
    "complex_text = \"Machine translation models are transforming multilingual communication.\"\n",
    "explore_tokenization(marian_tokenizer, complex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04265af1",
   "metadata": {},
   "source": [
    "## üöÄ Pushing the Boundaries: Recent Innovations\n",
    "\n",
    "Recent innovations in translation models include:\n",
    "\n",
    "1. **Multilingual Models**: Single models that can translate between many language pairs\n",
    "2. **Document-Level Translation**: Models that consider the broader context\n",
    "3. **Non-Autoregressive Translation**: Generating all output tokens in parallel\n",
    "4. **Adapter-Based Fine-tuning**: Efficiently adapting models to new language pairs\n",
    "5. **Prompt-Based Translation**: Using large language models with prompts for translation\n",
    "6. **Low-Resource Languages**: Specialized techniques for languages with limited data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f6f3e",
   "metadata": {},
   "source": [
    "## üîÑ Translation Quality and Evaluation\n",
    "\n",
    "Let's compare the quality of our translations from different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af10b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex examples to compare model outputs\n",
    "test_sentences = [\n",
    "    \"The sophisticated algorithm processes complex sentences with multiple clauses.\",\n",
    "    \"Neural networks have revolutionized machine translation in recent years.\",\n",
    "    \"The cat sat on the mat, watching the birds outside.\",\n",
    "    \"I would like to visit Berlin next summer if possible.\"\n",
    "]\n",
    "\n",
    "print(\"üîÑ Comparing translations between MarianMT and T5:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nExample {i+1}: {sentence}\")\n",
    "    \n",
    "    # MarianMT translation\n",
    "    inputs = marian_tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = marian_model.generate(**inputs, max_length=50)\n",
    "    marian_translation = marian_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # T5 translation\n",
    "    t5_input = \"translate English to German: \" + sentence\n",
    "    t5_inputs = t5_tokenizer(t5_input, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        t5_outputs = t5_model.generate(**t5_inputs, max_length=50)\n",
    "    t5_translation = t5_tokenizer.decode(t5_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nMarianMT: {marian_translation}\")\n",
    "    print(f\"T5:      {t5_translation}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a36868",
   "metadata": {},
   "source": [
    "## üß† Understanding the Limitations\n",
    "\n",
    "Translation models have several limitations to be aware of:\n",
    "\n",
    "1. **Context limitations**: Most models process sentences independently\n",
    "2. **Idiomatic expressions**: Difficult to translate culturally-specific phrases\n",
    "3. **Ambiguity**: Words with multiple meanings can be mistranslated\n",
    "4. **Low-resource languages**: Less accurate for languages with limited training data\n",
    "5. **Domain specificity**: Models may perform poorly on specialized content\n",
    "6. **Hallucinations**: Models can generate plausible but incorrect translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edb950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example demonstrating some limitations\n",
    "challenging_examples = [\n",
    "    \"The bat flew around the cave.\",  # Ambiguous word \"bat\"\n",
    "    \"It's raining cats and dogs outside.\",  # Idiomatic expression\n",
    "    \"I'll put the kettle on.\",  # Cultural expression\n",
    "    \"The patient presented with acute appendicitis.\"  # Domain-specific medical\n",
    "]\n",
    "\n",
    "print(\"üí≠ Examples demonstrating translation limitations:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(challenging_examples):\n",
    "    print(f\"\\nExample {i+1}: {sentence}\")\n",
    "    \n",
    "    # MarianMT translation\n",
    "    inputs = marian_tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = marian_model.generate(**inputs, max_length=50)\n",
    "    translation = marian_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Translation: {translation}\")\n",
    "    \n",
    "    if i == 0:\n",
    "        print(\"Note: 'Bat' could mean a flying mammal or a sports equipment - ambiguous.\")\n",
    "    elif i == 1:\n",
    "        print(\"Note: This is an idiomatic expression meaning 'It's raining heavily'.\")\n",
    "    elif i == 2:\n",
    "        print(\"Note: Cultural expression that might translate literally rather than idiomatically.\")\n",
    "    elif i == 3:\n",
    "        print(\"Note: Medical terminology may not be accurately translated without domain training.\")\n",
    "        \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb253c90",
   "metadata": {},
   "source": [
    "## üåü Conclusion\n",
    "\n",
    "In this notebook, we've explored:\n",
    "\n",
    "1. **Model Architectures**: The encoder-decoder transformer structure behind modern translation models\n",
    "2. **Translation Process**: How text flows through the model from input to output\n",
    "3. **Tokenization**: How models break text into subword units\n",
    "4. **Attention Mechanism**: The key innovation enabling high-quality translation\n",
    "5. **Model Comparison**: Differences between MarianMT and T5 approaches\n",
    "6. **Limitations**: Important considerations when using translation models\n",
    "\n",
    "These models demonstrate the power of transformers for natural language processing, particularly in the domain of machine translation. While they have limitations, they've dramatically improved translation quality compared to previous approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f11417",
   "metadata": {},
   "source": [
    "## üìö Further Reading\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original Transformer paper\n",
    "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) - T5 paper\n",
    "- [Marian: Fast Neural Machine Translation in C++](https://aclanthology.org/W18-2716.pdf) - Marian paper "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
